{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a21374b2-2df9-4910-bacf-deeea0ed44ae",
   "metadata": {},
   "source": [
    "# トークン化・変換・フィルタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "becd4020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde0d31",
   "metadata": {},
   "source": [
    "トークン化には spaCy を使います。\n",
    "まずは spaCy を import しておきましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f8902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 02:01:11.323480: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-09 02:01:11.323565: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a019d9-d1fa-47c3-b1c8-9cb0a37410a3",
   "metadata": {},
   "source": [
    "## 形態素解析\n",
    "\n",
    "spaCyを使うことで形態素解析とトークナイズを行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b961ce-c6bc-43ad-bde9-4cbe1445687a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[今日, は, 銀座, から, 池袋, へ, 向かい, まし, た, 。]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"ja_core_news_sm\")\n",
    "doc = nlp(\"今日は銀座から池袋へ向かいました。\")\n",
    "list(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd4fd5e-398f-40d7-b839-cf4e8874260f",
   "metadata": {},
   "source": [
    "## 見出し語（レンマ）化\n",
    "\n",
    "`.lemma_` 属性を参照して見出し語化できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22106737-f579-44a3-a3f7-ff7d3327b382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日 NOUN 今日\n",
      "は ADP は\n",
      "銀座 PROPN 銀座\n",
      "から ADP から\n",
      "池袋 PROPN 池袋\n",
      "へ ADP へ\n",
      "向かい VERB 向かう\n",
      "まし AUX ます\n",
      "た AUX た\n",
      "。 PUNCT 。\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93099cbe",
   "metadata": {},
   "source": [
    "## 品詞によるフィルタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395f1842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN 今日\n",
      "PROPN 銀座\n",
      "PROPN 池袋\n",
      "VERB 向かう\n"
     ]
    }
   ],
   "source": [
    "# 形態素の品詞の形態について書く (Universal Dependencies)\n",
    "# https://hironsan.hatenablog.com/entry/visualize-dependencies-per-bunsetsu\n",
    "doc = nlp(\"今日は銀座から池袋へ向かいました。\")\n",
    "\n",
    "# 内容語のみを残すようにフィルタリング\n",
    "for token in nlp(doc):\n",
    "    if token.pos_ in {\"NOUN\", \"PROPN\", \"VERB\"}:\n",
    "        print(token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a9ff08",
   "metadata": {},
   "source": [
    "## ストップワードによるフィルタリング\n",
    "\n",
    "ストップワードの集合を作成し、トークンがストップワードに含まれていれば出力しないようにします。\n",
    "ストップワードを考慮してトークナイズする関数を定義してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "178b779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, stopwords=set()):\n",
    "    return [t.lemma_ for t in nlp(text) if t.lemma_ not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b3b8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = {\n",
    "    \"は\", \"から\", \"へ\",\n",
    "    \"ます\", \"た\",\n",
    "    \"。\", \"、\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf99e542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['今日', '銀座', '池袋', '向かう']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"今日は銀座から池袋へ向かいました。\", stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a9d2b-28b2-4e58-b794-7f5f7361373a",
   "metadata": {},
   "source": [
    "## 小文字化\n",
    "\n",
    "strオブジェクトのlowerを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88ad1746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy spacy\n",
      "の の\n",
      "練習 練習\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"spaCyの練習\")\n",
    "for token in doc:\n",
    "    print(token.text, token.text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e13fa9-c670-4abf-8b44-de83fdaa6fbb",
   "metadata": {},
   "source": [
    "## 絵文字の変換\n",
    "\n",
    "SNSのデータを処理する際には絵文字が出現することが多いです。\n",
    "絵文字を事前に文字に変換しておくと処理しやすいです。\n",
    "\n",
    "絵文字の変換には[emoji](https://github.com/carpedm20/emoji/)パッケージが便利です。\n",
    "\n",
    "```{note}\n",
    "emojiパッケージを利用するにはインストールが必要です。\n",
    "\n",
    "    pip install emoji==1.7.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e7afcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1688a451",
   "metadata": {},
   "source": [
    "絵文字を文字列に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9f916ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今日はケーキを食べます:shortcake:'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.demojize(\"今日はケーキを食べます🍰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4c93ed",
   "metadata": {},
   "source": [
    "文字列を絵文字に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab50c349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今日はケーキを食べます🍰'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.emojize(\"今日はケーキを食べます:shortcake:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a786340d",
   "metadata": {},
   "source": [
    "絵文字を除去します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e53f311f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今日はケーキを食べます'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.replace_emoji(\"今日はケーキを食べます🍰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
